{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and save training outputs in a handy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import glob\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "from scipy.stats import norm, expon, chi2\n",
    "from scipy.stats import chisquare\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# open access to cernbox (password cern account)\n",
    "os.system(\"echo %s| kinit\" %getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_t(DIR_IN, DIR_OUT):\n",
    "    '''\n",
    "    For each toy the function reads the .txt file where the final value for the variable t=-2*loss is saved. \n",
    "    It then associates a label to each toy.\n",
    "    The array of the t values (tvalues) and the array of labels (files_id) are saved in an .h5 file.\n",
    "    \n",
    "    DIR_IN: directory where all the toys' outputs are saved\n",
    "    DIR_OUT: directory where to save the .h5 output file\n",
    "    \n",
    "    The function returns the array of labels.\n",
    "    '''\n",
    "    tvalues = np.array([])\n",
    "    files_id = np.array([])\n",
    "    FILE_TITLE=''\n",
    "    for fileIN in glob.glob(\"%s/*_t.txt\" %DIR_IN):\n",
    "        #print(fileIN)\n",
    "        f = open(fileIN)\n",
    "        lines = f.readlines()\n",
    "        file_id=  fileIN.split('/')[-1]\n",
    "        FILE_TITLE = fileIN.split('/')[-2]\n",
    "        file_id = file_id.replace('_t.txt', '')\n",
    "        #print(file_id)\n",
    "        if len(lines)==0:\n",
    "            continue\n",
    "        t = float(lines[0])\n",
    "        #print(file_id)\n",
    "        if(np.isnan(np.array([t]))): \n",
    "            continue    \n",
    "        tvalues = np.append(tvalues, t)\n",
    "        files_id = np.append(files_id, file_id)\n",
    "        \n",
    "    # save tvalues in a h5 file\n",
    "    f = h5py.File(DIR_OUT+ FILE_TITLE+'_tvalues.h5', 'w')\n",
    "    f.create_dataset('tvalues', data=tvalues, compression='gzip')\n",
    "    f.create_dataset('files_id', data=files_id, compression='gzip')\n",
    "    f.close()\n",
    "    \n",
    "    return files_id\n",
    "\n",
    "def collect_history(files_id, DIR_IN, patience):\n",
    "    '''\n",
    "    For each toy whose file ID is in the array files_id, \n",
    "    the function collects the history of the loss and saves t=-2*loss at the check points.\n",
    "    \n",
    "    files_id: array of toy labels \n",
    "    DIR_IN: directory where all the toys' outputs are saved\n",
    "    patience: interval between two check points (epochs)\n",
    "    \n",
    "    The function returns a 2D-array with final shape (nr toys, nr check points).\n",
    "    '''\n",
    "    tdistributions_check =np.array([])\n",
    "    cnt=0\n",
    "    for file_id in files_id:\n",
    "        history_file = DIR_IN+file_id+'_history'+str(patience)+'.h5'\n",
    "        #print(history_file)\n",
    "        f = h5py.File(history_file)\n",
    "        #print(history_file)\n",
    "        loss = f.get(\"loss\")\n",
    "        if not loss:\n",
    "            print(\"not\")\n",
    "            continue\n",
    "        loss = np.array(loss)\n",
    "        loss = np.expand_dims(loss, axis=1)\n",
    "        if not cnt:\n",
    "            # initialize the array at the first iteration\n",
    "            tdistributions_check = -2*loss\n",
    "        else:\n",
    "            # just append to tdistributions_check\n",
    "            tdistributions_check = np.concatenate((tdistributions_check, -2*loss), axis=1)\n",
    "        print(str(cnt)+': toy '+file_id+' loaded.')\n",
    "        cnt = cnt+1\n",
    "        #print(tdistributions_check.shape)\n",
    "    print('Final history array shape')\n",
    "    print('(nr toys, nr check points)')\n",
    "    print(tdistributions_check.T.shape)\n",
    "    return tdistributions_check.T\n",
    "\n",
    "    \n",
    "\n",
    "def Save_to_h5(DIR_OUT, file_name, extension, patience, tvalues_check):\n",
    "    '''\n",
    "    The function save the 2D-array of the loss histories in an .h5 file.\n",
    "    \n",
    "    DIR_OUT: directory where to save the output file\n",
    "    file_name: output file name\n",
    "    extension: label to be appended to the file_name\n",
    "    \n",
    "    No return.\n",
    "    '''\n",
    "    epochs_check = []\n",
    "    nr_check_points = tvalues_check.shape[1]\n",
    "    for i in range(nr_check_points):\n",
    "        epoch_check = patience*(i+1)\n",
    "        epochs_check.append(epoch_check)\n",
    "        \n",
    "    log_file = DIR_OUT+file_name+extension+'.h5' #'_tvalues_check.h5'\n",
    "    print(log_file)\n",
    "    f = h5py.File(log_file,\"w\")\n",
    "    for i in range(tvalues_check.shape[1]):\n",
    "        f.create_dataset(str(epochs_check[i]), data=tvalues_check[:, i], compression='gzip')\n",
    "    f.close()\n",
    "    print('Saved to file: ' +file_name+extension+'.h5')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directory\n",
    "output_path='./....'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# input directory\n",
    "DIR_INPUT = './....'\n",
    "\n",
    "# \n",
    "patience = DIR_INPUT.split(\"patience\",1)[1] \n",
    "patience = patience.split(\"_\",1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DIR_INPUT.endswith('/'):\n",
    "    DIR_INPUT=DIR_INPUT+'/'\n",
    "    \n",
    "title = DIR_INPUT.split('/')[-2]\n",
    "print(title)\n",
    "\n",
    "files_id = collect_t(DIR_INPUT, output_path)\n",
    "print('Loaded') \n",
    "tvalues_check = collect_history(files_id, DIR_INPUT, int(patience))\n",
    "\n",
    "Save_to_h5(output_path, title, '_tvalues_check', int(patience), tvalues_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_history_from_h5(DIR_IN, file_name, extension, patience, epochs):\n",
    "    '''\n",
    "    The function creates a 2D-array from a .h5 file.\n",
    "    \n",
    "    DIR_OUT: directory where to save the input file\n",
    "    file_name: input file name\n",
    "    extension: label to be appended to the file_name\n",
    "    \n",
    "    The function returns a 2D-array with final shape (nr toys, nr check points). \n",
    "    '''\n",
    "    \n",
    "    tvalues_check = np.array([])\n",
    "    epochs_check = []\n",
    "    \n",
    "    for i in range(epochs/patience):\n",
    "        epoch_check = patience*(i+1)\n",
    "        epochs_check.append(epoch_check)\n",
    "        \n",
    "    log_file = DIR_IN+file_name+extension+'.h5'\n",
    "    print(log_file)\n",
    "\n",
    "    f = h5py.File(log_file,\"r\")\n",
    "    for i in range(len(epochs_check)):\n",
    "        # the t distribution at each check point is named by the epoch number\n",
    "        t = f.get(str(epochs_check[i]))\n",
    "        t = np.array(t)\n",
    "        t = np.expand_dims(t, axis=1)\n",
    "        #print(t.shape)\n",
    "        if not i:\n",
    "            # initialize the array at the first iteration\n",
    "            tvalues_check = t\n",
    "        else:\n",
    "            # just append to tvalues_check\n",
    "            tvalues_check = np.concatenate((tvalues_check, t), axis=1)\n",
    "    f.close()\n",
    "    print(tvalues_check.shape)\n",
    "    return tvalues_check\n",
    "\n",
    "def Read_t_from_h5(DIR_IN, file_name, extension):\n",
    "    log_file = DIR_IN+file_name+extension+'.h5'\n",
    "    print(log_file)\n",
    "    tvalues_check = np.array([])\n",
    "    f = h5py.File(log_file,\"r\")\n",
    "    t = f.get('tvalues')\n",
    "    t = np.array(t)\n",
    "    print(t.shape)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Percentiles_v1(tvalues_check, patience, title='', ymax=300, ymin=0):\n",
    "    '''\n",
    "    The funcion creates the plot of the evolution in the epochs of the [2.5%, 25%, 50%, 75%, 97.5%] quantiles of the toy sample distribution.\n",
    "    \n",
    "    patience: interval between two check points (epochs).\n",
    "    tvalues_check: t=-2*loss\n",
    "    '''\n",
    "    epochs_check = []\n",
    "    nr_check_points = tvalues_check.shape[1]\n",
    "    for i in range(nr_check_points):\n",
    "        epoch_check = patience*(i+1)\n",
    "        epochs_check.append(epoch_check)\n",
    "    \n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    quantiles=[2.5, 25, 50, 75, 97.5]\n",
    "    percentiles=np.array([])\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('t', fontsize=12)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    for i in range(tvalues_check.shape[1]):\n",
    "        percentiles_i = np.percentile(tvalues_check[:, i], quantiles)\n",
    "        #print(percentiles_i.shape)\n",
    "        percentiles_i = np.expand_dims(percentiles_i, axis=1)\n",
    "        #print(percentiles_i.shape)\n",
    "        if not i:\n",
    "            percentiles = percentiles_i.T\n",
    "        else:\n",
    "            percentiles = np.concatenate((percentiles, percentiles_i.T))\n",
    "    legend=[]\n",
    "    print(percentiles.shape)\n",
    "    for j in range(percentiles.shape[1]):\n",
    "        plt.plot(epochs_check, percentiles[:, j], marker='.')\n",
    "        legend.append(str(quantiles[j])+' % quantile')\n",
    "    plt.legend(legend, fontsize=13)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig(output_path+title+'_PlotPercentiles.png')\n",
    "    plt.close(fig)\n",
    "    return\n",
    "\n",
    "def Plot_Percentiles_v2(tvalues_check, patience, title, dof, wc, ymax=300, ymin=0):\n",
    "    '''\n",
    "    The funcion creates the plot of the evolution in the epochs of the [2.5%, 25%, 50%, 75%, 97.5%] quantiles of the toy sample distribution.\n",
    "    \n",
    "    patience: interval between two check points (epochs).\n",
    "    tvalues_check: t=-2*loss\n",
    "    '''\n",
    "    epochs_check = []\n",
    "    nr_check_points = tvalues_check.shape[1]\n",
    "    for i in range(nr_check_points):\n",
    "        epoch_check = patience*(i+1)\n",
    "        epochs_check.append(epoch_check)\n",
    "    \n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    quantiles=[2.5, 25, 50, 75, 97.5]\n",
    "    percentiles=np.array([])\n",
    "    plt.xlabel('Training Epochs', fontsize=16)\n",
    "    plt.ylabel('t', fontsize=16)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.title('Weight Clipping = '+str(wc), fontsize=16)\n",
    "    for i in range(tvalues_check.shape[1]):\n",
    "        percentiles_i = np.percentile(tvalues_check[:, i], quantiles)\n",
    "        #print(percentiles_i.shape)\n",
    "        percentiles_i = np.expand_dims(percentiles_i, axis=1)\n",
    "        #print(percentiles_i.shape)\n",
    "        if not i:\n",
    "            percentiles = percentiles_i.T\n",
    "        else:\n",
    "            percentiles = np.concatenate((percentiles, percentiles_i.T))\n",
    "    legend=[]\n",
    "    #print(percentiles.shape)\n",
    "    for j in range(percentiles.shape[1]):\n",
    "        plt.plot(epochs_check, percentiles[:, j], marker='.', linewidth=3)\n",
    "        #print(percentiles[:, j])\n",
    "        legend.append(str(quantiles[j])+' % quantile')\n",
    "    for j in range(percentiles.shape[1]):\n",
    "        plt.plot(epochs_check, chi2.ppf(quantiles[j]/100., df=dof, loc=0, scale=1)*np.ones_like(epochs_check),\n",
    "                color='grey', ls='--', linewidth=1)\n",
    "        #print( chi2.ppf(quantiles[j]/100., df=dof, loc=0, scale=1))\n",
    "        if j==0:\n",
    "            legend.append(\"Target \"+r\"$\\chi^2(dof=$\"+str(dof)+\")\")\n",
    "            \n",
    "    plt.legend(legend, fontsize=16)\n",
    "    #plt.grid()\n",
    "    plt.show()\n",
    "    #fig.savefig(output_path+title+'_PlotPercentiles.png')\n",
    "    plt.close(fig)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_CHI2_tests(output_path, titleID, tdistributions_check, patience, \n",
    "                    xmin, xmax, bins, dof, \n",
    "                    shift=0, plot_patience=50,\n",
    "                    verbose=0, save=0):\n",
    "    '''\n",
    "    The function creates an array of plots for the distribution of t at different check points during the training.\n",
    "    \n",
    "    patience: interval between two check points (epochs).\n",
    "    tdistributions_check: 2Darray of the t distribution history.\n",
    "    '''\n",
    "    epochs_check = []\n",
    "    nr_check_points = tdistributions_check.shape[1]\n",
    "    print(nr_check_points)\n",
    "    print(patience)\n",
    "    for i in range(nr_check_points):\n",
    "        epoch_check = patience*(i+1)\n",
    "        epochs_check.append(epoch_check)\n",
    "    \n",
    "    # build the reference binned chi2 distribution starting from the cdf\n",
    "    chi_range = xmax-xmin\n",
    "    chi_bin = chi_range*1./bins\n",
    "    chi_ref = np.array([])\n",
    "    chi_xs = np.array([])\n",
    "    \n",
    "    for i in range(bins):\n",
    "        chi_x = xmin+0.5*(chi_bin*(i)+chi_bin*(i+1))+shift\n",
    "        chi_xs = np.append(chi_xs, chi_x)\n",
    "        chi_integral = (chi2.cdf(xmin+chi_bin*(i+1), dof)-chi2.cdf(xmin+chi_bin*(i), dof))\n",
    "        chi_ref= np.append(chi_ref, chi_integral)\n",
    "        \n",
    "    # setting the dimension of the plot grid    \n",
    "    nplots = int(len(epochs_check)/plot_patience)+(len(epochs_check)%plot_patience > 0)\n",
    "    ncols = 4\n",
    "    nrows = int(nplots/4)+ (nplots% 4 > 0)\n",
    "    fig_height = nrows*7\n",
    "    fig_width = ncols*5\n",
    "    fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "    \n",
    "    # building histograms\n",
    "    chi2_tests=[]\n",
    "    p_val_chi2_tests=[]\n",
    "    epoch_labels = []\n",
    "    for check in range(len(epochs_check)):\n",
    "        if check%plot_patience:\n",
    "            continue\n",
    "        plt.subplot(nrows, 4, check/plot_patience+1)\n",
    "        epoch = epochs_check[check]\n",
    "        epoch_labels.append(int(epoch/1000.))\n",
    "        \n",
    "        t = plt.hist(tdistributions_check[:, check], bins=bins, \n",
    "                     alpha=0.7, range=(xmin+shift, xmax+shift), edgecolor='blue')\n",
    "        chi_ref_lineplot = plt.errorbar(chi_xs, chi_ref*np.sum(t[0]), yerr=np.sqrt(chi_ref*np.sum(t[0])), \n",
    "                                 color='darkgreen')\n",
    "        plt.legend(['t distribution at '+str(epoch_labels[-1])+'k epochs', 'chi2 with '+str(dof)+' dof'],\n",
    "                   loc='upper center', bbox_to_anchor=(0.5, -0.1)) \n",
    "        \n",
    "        # building a chi2 test of compatibility between histogram and reference distribution\n",
    "        chi2_obs = t[0]\n",
    "        chi2_true = chi_ref*np.sum(t[0])\n",
    "        \n",
    "        numerator = np.multiply(np.subtract(chi2_obs, chi2_true), np.subtract(chi2_obs, chi2_true))\n",
    "        denominator = chi2_true\n",
    "        \n",
    "        chi2_test = np.sum(np.divide(numerator, denominator))\n",
    "        chi2_tests.append(chi2_test)\n",
    "        \n",
    "        p_val_chi2_test = 1.-chi2.cdf(chi2_test, bins-1)\n",
    "        p_val_chi2_tests.append(p_val_chi2_test)\n",
    "        props = dict(boxstyle='square', facecolor='white', alpha=0.1)\n",
    "        ylmin, ylmax = plt.ylim()\n",
    "        xlmin, xlmax = plt.xlim()\n",
    "        textstr = \"median: %f\" %(np.median(tdistributions_check[:, check].astype(float)))\n",
    "        \n",
    "        plt.annotate(textstr, xy=(0.2, 0.9), xycoords='axes fraction',\n",
    "                 #verticalalignment='top',horizontalalignment='right', \n",
    "                 fontsize=11)\n",
    "    plt.subplots_adjust(left=0.25, bottom = 0.5, wspace=0.3, hspace=0.5)\n",
    "    if verbose:\n",
    "        plt.show()\n",
    "    fig.savefig(output_path+titleID+'_hist_CHI2_dof'+str(dof)+'_'+str(xmin)+'-'+str(xmax)+'bins'+str(bins)+'.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # plot compatibility test results\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_labels, chi2_tests)\n",
    "    plt.plot([0, epoch_labels[-1]],[bins-1, bins-1])\n",
    "    plt.plot([0, epoch_labels[-1]],[bins-1+np.sqrt(2*(bins-1)), bins-1+np.sqrt(bins-1)])\n",
    "    plt.plot([0, epoch_labels[-1]],[bins-1+2*np.sqrt(2*(bins-1)), bins-1+2*np.sqrt(bins-1)])\n",
    "    plt.plot([0, epoch_labels[-1]],[bins-1+3*np.sqrt(2*(bins-1)), bins-1+3*np.sqrt(bins-1)])\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epochs [k]')\n",
    "    plt.ylabel('chi2 test')\n",
    "    plt.grid()\n",
    "    plt.legend(['chi2 obs','dof '+str(bins-1), 'dof + 1 sigma', 'dof + 2 sigma', 'dof + 3 sigma'])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_labels, p_val_chi2_tests)\n",
    "    plt.plot([0, epoch_labels[-1]],[0.05, 0.05])\n",
    "    plt.plot([0, epoch_labels[-1]],[0.32, 0.32])\n",
    "    plt.plot([0, epoch_labels[-1]],[0.5, 0.5])\n",
    "    plt.legend(['p-values', '0.05', '0.32', '0.5'])\n",
    "    plt.ylabel('p-value')\n",
    "    plt.xlabel('Epochs [k]')\n",
    "    #plt.yscale('log')\n",
    "    print(epoch_labels)\n",
    "    print(chi2_tests)\n",
    "    print(p_val_chi2_tests)\n",
    "    #save into .h5\n",
    "    log_file=output_path+'/'+titleID+'_CHI2_test.h5'\n",
    "    f = h5py.File(log_file,\"w\")\n",
    "    f.create_dataset(titleID.split('_')[-1], data=chi2_tests, compression='gzip')\n",
    "    f.create_dataset('epochs', data=epoch_labels, compression='gzip')\n",
    "    f.close()\n",
    "    plt.grid()\n",
    "    #plt.subplots_adjust(left=0.25, bottom = 0.3, wspace=0.3, hspace=0.3)\n",
    "    if verbose:\n",
    "        #plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.3, wspace=0.4)\n",
    "        plt.subplots_adjust(left=0.125, right=0.9, bottom = 0.25, wspace=0.3, hspace=0.2)\n",
    "        plt.show()\n",
    "    if save:\n",
    "        fig.savefig(output_path+titleID+'_plot_CHI2_dof'+str(dof)+'_'+str(xmin)+'-'+str(xmax)+'bins'+str(bins)+'.png')\n",
    "    plt.close(fig) \n",
    "    print(np.max(p_val_chi2_tests))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Analysis_tdistribution(output_path, title, tvalues_BkgOnly, tvalues, dof, rmin, rmax, bins=35, verbose=0, save=0):\n",
    "    '''\n",
    "    The function creates the plot for the comparison of two samples of toys at the end of the training.\n",
    "    tvalues_BkgOnly: t distribution for the sample with BKG-only events.\n",
    "    tvalues: t distribution for the sample with Sig+Bkg events.\n",
    "    dof: number of degrees of freedom of the reference chi2 distribution.\n",
    "    \n",
    "    '''\n",
    "    fig, ax = plt.subplots()\n",
    "    chisq = np.random.chisquare(dof, 5000000)\n",
    "    ax.hist(chisq, bins=bins, range=(rmin, rmax), density = True, histtype = 'step', linewidth=2, color='darkgreen')\n",
    "    ax.hist(tvalues_BkgOnly, bins=bins, range=(rmin, rmax), density = True, alpha = 0.7, edgecolor='blue')\n",
    "    ax.hist(tvalues, bins=bins, range=(rmin, rmax), density= True, alpha = 0.7, edgecolor='red')\n",
    "    ax.legend([\"$\\chi^2$ with \"+str(dof)+\" dof\",'Data samples following SM','Data samples containing New Physics'], loc='upper right')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_xlabel(\"t\")\n",
    "    ax.set_title(title)\n",
    "    #compute significance\n",
    "    #median = np.median(tvalues)\n",
    "    quantiles=np.percentile(tvalues, [16., 50., 84.])\n",
    "    q50=quantiles[1]\n",
    "    q16=quantiles[0]\n",
    "    q84=quantiles[2]\n",
    "    counts50 = np.sum((tvalues_BkgOnly > q50).astype(int))\n",
    "    counts16 = np.sum((tvalues_BkgOnly > q16).astype(int))\n",
    "    counts84 = np.sum((tvalues_BkgOnly > q84).astype(int))\n",
    "    \n",
    "    p_val50 = counts50*1./len(tvalues_BkgOnly)\n",
    "    p_val16 = counts16*1./len(tvalues_BkgOnly)\n",
    "    p_val84 = counts84*1./len(tvalues_BkgOnly)\n",
    "    \n",
    "    chisq = np.random.chisquare(dof, 100000000)\n",
    "    integral50 = (chisq > q50).sum()/float(len(chisq))\n",
    "    integral16 = (chisq > q16).sum()/float(len(chisq))\n",
    "    integral84 = (chisq > q84).sum()/float(len(chisq))\n",
    "    \n",
    "    print(\"Bkg-only median: %f\" %np.median(tvalues_BkgOnly))\n",
    "    print(\"Bkg-only mean: %f\" %np.mean(tvalues_BkgOnly))\n",
    "    print(\"Bkg-only RMS: %f\" %math.sqrt(np.var(tvalues_BkgOnly)))\n",
    "    print(\"Sig+Bkg median: %f\" %np.median(tvalues))\n",
    "    print(\"Sig+Bkg quantile16: %f\" %q16)\n",
    "    print(\"Sig+Bkg quantile84: %f\" %q84)\n",
    "    print(\"Sig+Bkg mean: %f\" %np.mean(tvalues))\n",
    "    print(\"Sig+Bkg RMS: %f\" %math.sqrt(np.var(tvalues)))\n",
    "    \n",
    "    print(\"p-value %f with 68 %% CL [%f, %f]\" %(p_val50, p_val16, p_val84))\n",
    "    print(\"number of sigmas: %f with 68%% CL [%f, %f]\" %(norm.ppf(1.-p_val50), norm.ppf(1.-p_val16), norm.ppf(1.-p_val84)))\n",
    "    print(\"p-value assuming %i dof chi square: %f\" %(dof, integral50))\n",
    "    print(\"number of sigmas assuming %i dof chi square: %f with 68 %% CL [%f, %f]\" %(dof, norm.ppf(1.-integral50), norm.ppf(1.-integral16), norm.ppf(1.-integral84)))\n",
    "    textstr = \"Bkg-only median: %f\\nSig+Bkg median: %f\\nSignificance: %f $\\sigma$\\nTh Significance: %f $\\sigma$\" %(np.median(tvalues_BkgOnly), np.median(tvalues), norm.ppf(1.-p_val50), norm.ppf(1.-integral50))\n",
    "\n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='square', facecolor='white', alpha=0.1)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax.text(0.5, 0.65, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "    if verbose:\n",
    "        plt.show()\n",
    "    fig.savefig(output_path+title+'_t_distributions_dof'+str(dof)+'.png')\n",
    "    plt.close(fig)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save plots\n",
    "output_path='/...'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# where you saved *tvalues_check.h5 and *tvalues.h5\n",
    "read_path_B='/...'\n",
    "read_path_SB='/....'\n",
    "\n",
    "\n",
    "# Collect training parameters from folder name\n",
    "\n",
    "# where you saved the training outputs\n",
    "DIR_INPUT_B = '..../Z_5D_patience10000_ref1000000_bkg200000_sig0_epochs600000_latent13_layers1_wclip0.7/'\n",
    "DIR_INPUT_SB ='..../Z_5D_ref1000000_bkg200000_sig100_epochs300000_latent5_layers3_wclip2.15/'\n",
    "\n",
    "patience_B = DIR_INPUT_B.split(\"patience\",1)[1] \n",
    "patience_B = patience_B.split(\"_\",1)[0]\n",
    "patience_SB = DIR_INPUT_SB.split(\"patience\",1)[1] \n",
    "patience_SB = patience_SB.split(\"_\",1)[0]\n",
    "\n",
    "wclip_B = DIR_INPUT_B.split(\"wclip\",1)[1] \n",
    "wclip_B = wclip_B.split(\"_\",1)[0]\n",
    "wclip_B = wclip_B.split(\"/\",1)[0]\n",
    "wclip_SB = DIR_INPUT_SB.split(\"wclip\",1)[1] \n",
    "wclip_SB = wclip_SB.split(\"_\",1)[0]\n",
    "wclip_SB = wclip_SB.split(\"/\",1)[0]\n",
    "\n",
    "epochs_B = DIR_INPUT_B.split(\"epochs\",1)[1] \n",
    "epochs_B = epochs_B.split(\"_\",1)[0]\n",
    "epochs_SB = DIR_INPUT_SB.split(\"epochs\",1)[1] \n",
    "epochs_SB = epochs_SB.split(\"_\",1)[0]\n",
    "\n",
    "if not DIR_INPUT_B.endswith('/'):\n",
    "    DIR_INPUT_B = DIR_INPUT_B+'/'\n",
    "if not DIR_INPUT_SB.endswith('/'):\n",
    "    DIR_INPUT_SB = DIR_INPUT_SB+'/'\n",
    "\n",
    "title_SB = DIR_INPUT_SB.split('/')[-2]\n",
    "title_B = DIR_INPUT_B.split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading loss history\n",
    "tvalues_check_B = Read_history_from_h5(read_path_B, title_B, '_tvalues_check', int(patience_B), int(epochs_B))\n",
    "tvalues_check_SB = Read_history_from_h5(read_path_SB, title_SB, '_tvalues_check', int(patience_SB), int(epochs_SB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading of the final t distributions\n",
    "t_B = Read_t_from_h5(read_path_B, title_B, extension='_tvalues')\n",
    "t_SB = Read_t_from_h5(read_path_SB, title_SB, extension='_tvalues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### percentile plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Percentiles_v2(tvalues_check_B, int(patience_B), title_B, dof=10, wc=float(wclip_B), ymax=40, ymin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHI2 test of the compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_CHI2_tests(output_path, title_B, tvalues_check_B, \n",
    "                patience=10000,\n",
    "                xmin=10, xmax=70, bins=7,\n",
    "                dof=40, shift=0, \n",
    "                plot_patience=1, verbose=1, save=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test statistic on BSM events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Analysis_tdistribution(output_path, title_SB, t_B, t_SB, dof=96, rmin=50, rmax=667, bins=15, verbose=1, save=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
