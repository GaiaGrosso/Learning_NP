{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, expon, chi2, uniform\n",
    "from scipy.stats import chisquare\n",
    "from scipy.optimize import leastsq, curve_fit, least_squares\n",
    "import getpass\n",
    "os.system(\"echo %s| kinit\" %getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras import callbacks\n",
    "from keras import metrics, losses, optimizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Input, Conv1D, Flatten, Dropout, LeakyReLU, Layer\n",
    "from keras.constraints import Constraint, max_norm\n",
    "\n",
    "class WeightClip(Constraint):\n",
    "    '''Clips the weights incident to each hidden unit to be inside a range                                                                        \\\n",
    "                                                                                                                                                   \n",
    "    '''\n",
    "    def __init__(self, c=2):\n",
    "        self.c = c\n",
    "    def __call__(self, p):\n",
    "        return K.clip(p, -self.c, self.c)\n",
    "    def get_config(self):\n",
    "        return {'name': self.__class__.__name__,\n",
    "                'c': self.c}\n",
    "\n",
    "def MyModel(nInput, latentsize, layers, weight_clipping):\n",
    "    inputs = Input(shape=(nInput, ))\n",
    "    dense  = Dense(latentsize, input_shape=(nInput,), activation='sigmoid', W_constraint = WeightClip(weight_clipping))(inputs)\n",
    "    for l in range(layers-1):\n",
    "        dense  = Dense(latentsize, input_shape=(latentsize,), activation='sigmoid', W_constraint = WeightClip(weight_clipping))(dense)\n",
    "    output = Dense(1, input_shape=(latentsize,), activation='linear', W_constraint = WeightClip(weight_clipping))(dense)\n",
    "    model = Model(inputs=[inputs], outputs=[output])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildSample_DY(N_Events, INPUT_PATH, seed, nfiles=20):\n",
    "    #random integer to select Zprime file between n files                                                                                                                                                             \n",
    "    u = np.arange(nfiles)#np.random.randint(100, size=100)                                                                                                                                                            \n",
    "    np.random.shuffle(u)\n",
    "\n",
    "    #BACKGROUND                                                                                                                                                                                                       \n",
    "    #extract N_Events from files                                                                                                                                                                                      \n",
    "    toy_label = INPUT_PATH.split(\"/\")[-2]\n",
    "    print(toy_label)\n",
    "\n",
    "    HLF = np.array([])\n",
    "\n",
    "    for u_i in u:\n",
    "        f = h5py.File(INPUT_PATH+toy_label+str(u_i+1)+\".h5\")\n",
    "        keys=f.keys()\n",
    "        #check whether the file is empty                                                                                                                                                                              \n",
    "        if len(keys)==0:\n",
    "            continue\n",
    "        cols=np.array([])\n",
    "        for i in range(len(keys)):\n",
    "            feature = np.array(f.get(keys[i]))\n",
    "            feature = np.expand_dims(feature, axis=1)\n",
    "            if i==0:\n",
    "                cols = feature\n",
    "            else:\n",
    "                cols = np.concatenate((cols, feature), axis=1)\n",
    "        print(cols.shape)\n",
    "        np.random.shuffle(cols) #don't want to select always the same event first                                                                                                                                     \n",
    "\n",
    "        if HLF.shape[0]==0:\n",
    "            HLF=cols\n",
    "            i=i+1\n",
    "        else:\n",
    "            HLF=np.concatenate((HLF, cols), axis=0)\n",
    "        f.close()\n",
    "        #print(HLF_REF.shape)                                                                                                                                                                                         \n",
    "        if HLF.shape[0]>=N_Events:\n",
    "            HLF=HLF[:N_Events, :]\n",
    "            break                                                                                                                                                                                               \n",
    "    print(HLF.shape)\n",
    "    # feature order: pt1, pt2, eta1, eta2, delta_phi, mass\n",
    "    return HLF[:, [4, 5, 1, 2, 0, 3]]\n",
    "\n",
    "def collect_predictions(DIR_IN):\n",
    "    '''\n",
    "    collect predictions from file.\n",
    "    collect seeds from filename.\n",
    "    collect t values from file.\n",
    "    '''\n",
    "    predictions_set = np.array([])\n",
    "    seeds_set       = np.array([])\n",
    "    toy_labels_set  = np.array([])\n",
    "    t_set           = np.array([])\n",
    "    cnt = 0\n",
    "    for fileIN in glob.glob(\"%s/*_predictions.h5\" %DIR_IN):\n",
    "        # collect seed\n",
    "        seed = fileIN.split('seed', 1)[1]\n",
    "        seed = int(seed.split('_', 1)[0])\n",
    "        print(seed)\n",
    "        seeds_set = np.append(seeds_set, seed)\n",
    "        \n",
    "        # collect toy label ID\n",
    "        toy_label = fileIN.split('_')[-3]\n",
    "        toy_labels_set = np.append(toy_labels_set, toy_label)\n",
    "        \n",
    "        # collect t\n",
    "        file_t = fileIN.split('seed', 1)[0]\n",
    "        file_t = file_t+'t.txt'\n",
    "        f_t    = open(file_t)\n",
    "        lines  = f_t.readlines()\n",
    "        if len(lines)==0:\n",
    "            continue\n",
    "        t      =  float(lines[0])\n",
    "        if(np.isnan(np.array([t]))):\n",
    "            t  = 0\n",
    "        t_set  = np.append(t_set, t)\n",
    "        f_t.close()\n",
    "        \n",
    "        # collect predictions\n",
    "        f = h5py.File(fileIN)\n",
    "        predictions = f.get(\"predictions\")\n",
    "        target = f.get(\"target\")\n",
    "        if not predictions:\n",
    "            print('continue')\n",
    "            continue\n",
    "        predictions = np.array(predictions)\n",
    "        target      = np.array(target)\n",
    "        target      = np.expand_dims(target, axis=1)\n",
    "        if not cnt:\n",
    "            # initialize the array at the first iteration\n",
    "            print(predictions.shape)\n",
    "            predictions_set = predictions\n",
    "        else:\n",
    "            # just append to tdistributions_check\n",
    "            predictions_set = np.concatenate((predictions_set, predictions), axis=1)\n",
    "        print(str(cnt)+': toy '+toy_label+' loaded.')\n",
    "        cnt += 1\n",
    "\n",
    "    print('Final predictions array shape')\n",
    "    print('(predictions, nr toys)')\n",
    "    print(predictions_set.shape)\n",
    "    return predictions_set, seeds_set, toy_labels_set, t_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH_BKG = './DiLepton_SM/'\n",
    "INPUT_PATH_SIG = './BSM_Detection/DiLepton_Zprime300/\n",
    "nfile_REF = 66\n",
    "nfile_SIG = 1\n",
    "\n",
    "\n",
    "output_path='./WhatNN_haslearnt/Zprime300/'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "\n",
    "DIR_INPUT ='.../Z_5D_patience10000_EFT06_ref100000_data20053_epochs300000_latent5_layers3_wclip1.9/'\n",
    "\n",
    "patience = DIR_INPUT.split(\"patience\",1)[1] \n",
    "patience = int(patience.split(\"_\",1)[0])\n",
    "\n",
    "epochs = DIR_INPUT.split(\"epochs\",1)[1] \n",
    "epochs = int(epochs.split(\"_\",1)[0])\n",
    "\n",
    "wclip = DIR_INPUT.split(\"wclip\",1)[1] \n",
    "wclip = float(wclip.split(\"_\",1)[0][:-1])\n",
    "\n",
    "Nsig = 0\n",
    "if \"sig\" in DIR_INPUT.split('/')[-2]:\n",
    "    Nsig = DIR_INPUT.split(\"sig\",1)[1] \n",
    "    Nsig = int(Nsig.split(\"_\",1)[0]) \n",
    "\n",
    "Nbkg = 0\n",
    "if \"bkg\" in DIR_INPUT.split('/')[-2]:\n",
    "    print(DIR_INPUT.split('/')[-1])\n",
    "    Nbkg = DIR_INPUT.split(\"bkg\",1)[1] \n",
    "    Nbkg = int(Nbkg.split(\"_\",1)[0] )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Signal reference distribution with all the signal events at our disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a signal refernce dataset\n",
    "Nsig_TOT = 80000\n",
    "seed_TOT = datetime.datetime.now().microsecond+datetime.datetime.now().second+datetime.datetime.now().minute\n",
    "if not cut and Nsig:\n",
    "    # Zprime with no cut\n",
    "    # build signal\n",
    "    HLF_SIG, target_SIG, sig_target_SIG = BuildSample_DY(Nsig_TOT, INPUT_PATH_SIG, seed_TOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the predictions output for a set of trained NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the seeds are stored as well to rebuild the training data (in particular the reference) of each case\n",
    "predictions_list, seeds_list, toy_labels_list, t_list = collect_predictions(DIR_INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the mass reconstruction made by the NN training and compare it to the refernce mass distribution (BKG only) and the target mass distribution (BGK+SIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_whatNNhaslearnt(predictions_list, seeds_list, toy_labels_list, t_list, \n",
    "                         Nsig, Nbkg, Nref, INPUT_PATH_BKG, INPUT_PATH_SIG, nfile_REF, nfile_SIG\n",
    "                        save_plot=False):\n",
    "    for i in range(seeds_list.shape[0]):\n",
    "        # reconstruct training data from seed\n",
    "        N_Sig_p = np.random.poisson(lam=Nsig, size=1)[0]\n",
    "        N_Bkg_p = np.random.poisson(lam=Nbkg, size=1)[0]\n",
    "\n",
    "        #BACKGROUND+REFERENCE\n",
    "        HLF_REF = BuildSample_DY(N_Events=Nref+N_Bkg_p, INPUT_PATH=INPUT_PATH_BKG, seed=int(seeds_list[i]), nfiles=nfile_REF)\n",
    "        #SIGNAL                                                                                                                                                                                                                \n",
    "        HLF_SIG = BuildSample_DY(N_Events=N_Sig_p, INPUT_PATH=INPUT_PATH_SIG, seed=int(seeds_list[i]), nfiles=nfile_SIG)\n",
    "        #TARGETS\n",
    "        target_REF  = np.zeros(N_ref)\n",
    "        target_DATA = np.ones(N_Bkg_p+N_Sig_p)\n",
    "        target      = np.append(target_REF, target_DATA)\n",
    "        target      = np.expand_dims(target, axis=1)\n",
    "        feature     = np.concatenate((HLF_REF, HLF_SIG), axis=0)\n",
    "        sig_target  = np.append(np.zeros(Nref+N_Bkg_p), np.ones(N_Sig_p))\n",
    "\n",
    "        #selecting ref events for the invariant mass (last column in feature)\n",
    "        ref_mass_masked  = np.ma.masked_where(target==1, feature[:, -1])\n",
    "        ref_mass         = np.ma.compressed(ref_mass_masked)\n",
    "        #selecting data events for the invariant mass\n",
    "        data_mass_masked = np.ma.masked_where(target==0, feature[:, -1])\n",
    "        data_mass        = np.ma.compressed(data_mass_masked)\n",
    "        #selecting ref predictions of the model\n",
    "        ref_predictions_masked = np.ma.masked_where(target==1, predictions_list[:, i])\n",
    "        ref_predictions        = np.ma.compressed(ref_predictions_masked)\n",
    "\n",
    "        fig=plt.figure(figsize=(10, 10))\n",
    "\n",
    "        \n",
    "        # weights \n",
    "        ref_weights        = np.ones_like(ref_mass)*Nbkg*1./Nref\n",
    "        retrieved_weights  = np.exp(ref_predictions)*(Nbkg)*1./Nref\n",
    "        difference_weights = (retrieved_weights - ref_weights)*Nbkg\n",
    "        sig_weights        = Nsig*np.ones_like(HLF_SIG[:, -1])*1./HLF_SIG.shape[0]\n",
    "        bkg_weights        = ref_weights\n",
    "        merged_weights     = np.concatenate((bkg_weights,sig_weights), axis=0) \n",
    "        data_weights       = np.ones_like(data_mass)#*(Nbkg_SB+Nsig_SB)*1./Nref_SB\n",
    "        \n",
    "        # building the retrieved distribution and compare it to the reference\n",
    "        plt.subplot(2, 2, 1)\n",
    "        bins = np.append(np.linspace(0, 400, 8), np.linspace(500, 2000, 3))\n",
    "        mass_ref_hist = plt.hist(ref_mass, weights=ref_weights, \n",
    "                                range=(0, 2000), bins=bins, \n",
    "                                alpha=0.5, \n",
    "                                label='REF', linewidth=2)\n",
    "        mass_retrieved_hist = plt.hist(ref_mass, weights=retrieved_weights,\n",
    "                                        range=(0, 2000), bins=bins,  \n",
    "                                        label='DATA RECO', linewidth=3, histtype='step')\n",
    "        \n",
    "        plt.title('Toy '+toy_labels_list[i]+': t = '+str(t_SB[i]))\n",
    "        plt.legend()\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.xlabel('Z mass (GeV)')\n",
    "\n",
    "        # highlight differences between reco and ref computing the difference  \n",
    "        plt.subplot(2, 2, 2)\n",
    "        bins = np.append(np.linspace(0, 400, 8), np.linspace(500, 2000, 3))\n",
    "        plt.hist(ref_mass, weights=difference_weights, \n",
    "                 range=(0, 2000), bins=bins,\n",
    "                label='DATA RECO - BKG', linewidth=3, histtype='step')\n",
    "        plt.hist(ref_mass, weights=difference_weights, \n",
    "                 range=(bins[4], bins[7]), bins=bins[4:8], alpha=0.5, \n",
    "                label='(200-400) GeV')\n",
    "        \n",
    "        plt.ylabel('Counts')\n",
    "        plt.xlabel('Z mass (GeV)')\n",
    "        plt.title('Toy '+toy_labels_SB[i]+': t = '+str(t_SB[i]))\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "\n",
    "        # building the retrieved distribution and compare it to the data\n",
    "        plt.subplot(2, 2, 3)\n",
    "        bins = np.append(np.linspace(0, 400, 8), np.linspace(500, 2000, 3))\n",
    "\n",
    "        mass_SIGref_hist = plt.hist(np.concatenate((ref_mass, HLF_SIG[:, -1]), axis=0), weights=merged_weights,\n",
    "                                    bins=bins, \n",
    "                                    alpha=0.5, color='orange',\n",
    "                                    label='BKG+SIG distribution', linewidth=2)\n",
    "        mass_data_hist = plt.hist(data_mass, weights=data_weights,  \n",
    "                                    bins=bins, color='red',\n",
    "                                    label='DATA', linewidth=2, histtype='step')\n",
    "        mass_retrieved_hist = plt.hist(ref_mass, weights=retrieved_weights,\n",
    "                                    bins=bins, \n",
    "                                    label='DATA RECO', linewidth=2, histtype='step')\n",
    "        plt.legend()\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.xlabel('Z mass (GeV)')\n",
    "\n",
    "        # building the ratio between data abd data reco\n",
    "        plt.subplot(2, 2, 4)\n",
    "        mass_data_counts      = mass_data_hist[0]\n",
    "        mass_ref_counts       = mass_ref_hist[0]\n",
    "        mass_retrieved_counts = mass_retrieved_hist[0]\n",
    "        mass_SIGref_counts    = mass_SIGref_hist[0]\n",
    "        ratio_reco   = np.array([])\n",
    "        ratio_data   = np.array([])\n",
    "        ratio_refSIG = np.array([])\n",
    "        x = np.array([])\n",
    "        for j in range(len(bins)-1):\n",
    "            x_j = 0.5*(bins[j+1]+bins[j])\n",
    "            if mass_ref_counts[j]!=0:\n",
    "                x = np.append(x, x_j)\n",
    "                ratio_reco = np.append(ratio_reco, (mass_retrieved_counts[j]/mass_ref_counts[j]))\n",
    "                ratio_data = np.append(ratio_data, (mass_data_counts[j]/mass_ref_counts[j]))\n",
    "                ratio_refSIG = np.append(ratio_refSIG, (mass_SIGref_counts[j]/mass_ref_counts[j]))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        plt.scatter(x, ratio_data, color='red', label='DATA / REF')\n",
    "        plt.scatter(x, ratio_reco, color='blue', label='RECO / REF')\n",
    "        plt.scatter(x, ratio_refSIG, color='orange', label='(BKG+SIG) / REF')\n",
    "        plt.plot(x, ratio_data, color='red', linewidth=2)\n",
    "        plt.plot(x, ratio_reco, color='blue', linewidth=3)\n",
    "        plt.plot(x, ratio_refSIG, color='orange', linewidth=3)\n",
    "        plt.legend()\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.xlabel('Z mass (GeV)')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplots_adjust(left=0.125, right=0.9, bottom = 0.25, wspace=0.3, hspace=0.2)\n",
    "        plt.show()\n",
    "        if save_plot:\n",
    "            fig.savefig(output_path+'toy_'+toy_labels_list[i]+'_reco.png')\n",
    "        plt.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
